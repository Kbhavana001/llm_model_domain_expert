{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll deploy the Meta Llama 2 7B model and evaluate it's text generation capabilities and domain knowledge. we'll use the SageMaker Python SDK for Foundation Models and deploy the model for inference. \n",
    "\n",
    "The Llama 2 7B Foundation model performs the task of text generation. It takes a text string as input and predicts next words in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up\n",
    "There are some initial steps required for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. we'll use the execution role associated with the current notebook instance as the AWS account role with SageMaker access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticating with AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::674482793186:role/service-role/SageMaker-udacitySagemakerRole\n",
      "us-west-2\n",
      "<sagemaker.session.Session object at 0x7f86a485fa90>\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "print(aws_role)\n",
    "print(aws_region)\n",
    "print(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting The Text Generation Model Meta Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(model_id, model_version,) = (\"meta-textgeneration-llama-2-7b\",\"2.*\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell deploys the model\n",
    "This Python code is used to deploy a machine learning model using Amazon SageMaker's JumpStart library. \n",
    "\n",
    "1. Import the `JumpStartModel` class from the `sagemaker.jumpstart.model` module.\n",
    "\n",
    "2. Create an instance of the `JumpStartModel` class using the `model_id` and `model_version` variables created in the previous cell. This object represents the machine learning model you want to deploy.\n",
    "\n",
    "3. Call the `deploy` method on the `JumpStartModel` instance. This method deploys the model on Amazon SageMaker and returns a `Predictor` object.\n",
    "\n",
    "The `Predictor` object (`predictor`) can be used to make predictions with the deployed model. The `deploy` method will automatically choose an endpoint name, instance type, and other deployment parameters. If you want to specify these parameters, you can pass them as arguments to the `deploy` method.\n",
    "\n",
    "**The next cell will take some time to run.**  It is deploying a large language model, and that takes time.  You'll see dashes (--) while it is being deployed.  Please be patient! You'll see an exclamation point at the end of the dashes (---!) when the model is deployed and then you can continue running the next cells.  \n",
    "\n",
    "You might see a warning \"For forward compatibility, pin to model_version...\" You can ignore this warning, just wait for the model to deploy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the Llama2 Model on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using vulnerable JumpStart model 'meta-textgeneration-llama-2-7b' and version '2.1.8'.\n",
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '2.*'. You can pin to version '2.1.8' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version, instance_type=\"ml.g5.2xlarge\")\n",
    "predictor = model.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the endpoint, query and parse response\n",
    "The next step is to invoke the model endpoint, send a query to the endpoint, and recieve a response from the model. \n",
    "\n",
    "Running the next cell defines a function that will be used to parse and print the response from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes a text string as input and predicts next words in the sequence, the input we send it is the prompt. \n",
    "\n",
    "The prompt we send the model should relate to the domain we'd like to fine-tune the model on.  This way we'll identify the model's domain knowledge before it's fine-tuned, and then we can run the same prompts on the fine-tuned model.   \n",
    "\n",
    "**Replace \"inputs\"** in the next cell with the input to send the model based on the domain you've chosen. \n",
    "\n",
    "**For financial domain:**\n",
    "\n",
    "  \"inputs\": \"Replace with sentence below\"  \n",
    "- \"The  investment  tests  performed  indicate\"\n",
    "- \"the  relative  volume  for  the  long  out  of  the  money  options, indicates\"\n",
    "- \"The  results  for  the  short  in  the  money  options\"\n",
    "- \"The  results  are  encouraging  for  aggressive  investors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Pre-trained Llama2 Text Generation Large Language Model for Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  results  are  encouraging  for  aggressive  investors\n",
      "> \n",
      "in  the  near-term  and  for  long-term  investors  as  well.\n",
      "The  results  are  encouraging  for  aggressive  investors  in\n",
      "the  near-term  and  for  long-term  investors  as \n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "payload = {\n",
    "    \"inputs\": \"The  results  are  encouraging  for  aggressive  investors\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - 1:\n",
    "\n",
    "  Input Prompt : \"The results for the short in the money options\"\n",
    "  \n",
    "  Output text generated : The stock is down 24% from its 52-week high. However, itâ€™s still up 22% from its 52-week low.\n",
    "The company is facing some headwinds in the near term, but it has a long-term growth plan that should\n",
    "\n",
    "### Example - 2:\n",
    "\n",
    "  Input Prompt : \"the relative volume for the long out of the money options, indicates\"\n",
    "  \n",
    "  output text generated :  that the options are trading at a 16.9% premium to the at the money options.\n",
    "The implied volatility for the long out of the money options is 29.2%.\n",
    "The implied volatility for the short out of the money options is 31\n",
    "  \n",
    "### Example - 3:\n",
    "\n",
    "  Input Prompt : \"The  investment  tests  performed  indicate\"\n",
    "  \n",
    "  output text generated : that  the  Company  has  adequate  liquidity  to  fund  its  operations\n",
    "through  the  foreseeable  future.\n",
    "In  addition,  the  Company  has  a  $250,000,000 revolving credit\n",
    "\n",
    "### Example - 4:\n",
    " \n",
    "  Input prompt : \"The  results  are  encouraging  for  aggressive  investors\"\n",
    "  \n",
    "  output text generated : in  the  near-term  and  for  long-term  investors  as  well.\n",
    "The  results  are  encouraging  for  aggressive  investors  in\n",
    "the  near-term  and  for  long-term  investors  as \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt is related to the domain you want to fine-tune your model on. You will see the outputs from the model without fine-tuning are limited in providing insightful or relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
